CUDA_VISIBLE_DEVICES=1,2 python prompt_eval.py \
  model.name="meta-llama/Meta-Llama-3-8B-Instruct" \
  model.num_devices=1 \
  data.direction="1_to_0" \
  data.max_size=100 \
  pruning.algorithm="PromptQuine" \
  prompt_quine.algorithm_mode=GGA \
  prompt_quine.initialize_duplicate=True \
  prompt_quine.successive_halving=True \
  model.ICL_index=0 \
  model.ICL_shots=1 \
  model.task_top_k=1 \
  model.num_samples=1

CUDA_VISIBLE_DEVICES=1,2 python prompt_eval.py \
  model.name="meta-llama/Meta-Llama-3-8B-Instruct" \
  model.num_devices=1 \
  data.direction="0_to_1" \
  data.max_size=100 \
  pruning.algorithm="PromptQuine" \
  prompt_quine.algorithm_mode=GGA \
  prompt_quine.initialize_duplicate=True \
  prompt_quine.successive_halving=True \
  model.ICL_index=0 \
  model.ICL_shots=1 \
  model.task_top_k=1 \
  model.num_samples=1

CUDA_VISIBLE_DEVICES=1,2 python prompt_eval.py \
  model.name="openai-community/gpt2" \
  model.num_devices=1 \
  data.direction="1_to_0" \
  data.max_size=100 \
  pruning.algorithm="PromptQuine" \
  prompt_quine.algorithm_mode=GGA \
  prompt_quine.initialize_duplicate=True \
  prompt_quine.successive_halving=True \
  model.ICL_index=0 \
  model.ICL_shots=1 \
  model.task_top_k=1 \
  model.num_samples=1

CUDA_VISIBLE_DEVICES=1,2 python prompt_eval.py \
  model.name="openai-community/gpt2" \
  model.num_devices=1 \
  data.direction="0_to_1" \
  data.max_size=100 \
  pruning.algorithm="PromptQuine" \
  prompt_quine.algorithm_mode=GGA \
  prompt_quine.initialize_duplicate=True \
  prompt_quine.successive_halving=True \
  model.ICL_index=0 \
  model.ICL_shots=1 \
  model.task_top_k=1 \
  model.num_samples=1
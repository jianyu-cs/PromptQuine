import sys
from typing import Optional, Tuple, List, Any, Dict

import ray
import pdb
import hydra
import numpy as np
import torch
from transformers import AutoTokenizer
from vllm import LLM, SamplingParams
from ray.util.placement_group import PlacementGroup
from ray.util.scheduling_strategies import PlacementGroupSchedulingStrategy

from metrics import jailbreakingMetricEvaluator
from steering_vector import SteeringVectorModelLayerWrapper

@ray.remote(num_gpus=1, num_cpus=4)
class JailbreakingPromptedModel:
    """Evaluator for prompted jailbreaking attacks (Inference-Only)."""
    def __init__(
        self,
        task_lm: str,
        dataset: str,
        num_devices: int,
    ):
        super().__init__()
        self.dataset = dataset
        self.task_lm = task_lm
        print("Task LM:", self.task_lm)
        
        self._tokenizer = AutoTokenizer.from_pretrained(
            self.task_lm, pad_token='<|endoftext|>')
        self._generator = LLM(task_lm, tensor_parallel_size=1, dtype="half")
        # self._generator.config.pad_token_id = self._tokenizer.pad_token_id

    @torch.no_grad()
    def forward(
        self,
        data: List[str],
        **kwargs
    ) -> List[str]:
        """Only support vLLM here.
        
        Args:
            data (List[str]): Formatted prompts.
            
        Returns:
            generated_texts (List[str]): Texts generated by our models.
        """ 
        # vLLM configurations
        stop_tokens = ["<|endoftext|>", "Question"]
        sampling_params = SamplingParams(temperature=0., max_tokens=512, n=1, stop=stop_tokens)
        # vLLM inference
        outputs = self._generator.generate(data, sampling_params)
        # Output texts
        output_texts = [output.outputs[0].text for output in outputs]
        return output_texts

    @torch.no_grad()
    def forward_batch(
        self,
        data: List[str],
        num_samples_per_prompt: int,
        **kwargs
    ) -> List[List[str]]:
        """
        Batch evaluation using vLLM for multiple prompts.

        Args:
            data (List[str]): List of input prompts or queries (prompt formatted).
            num_samples_per_prompt (int): Number of samples to generate per prompt.
            **kwargs: Additional keyword arguments for vLLM inference.

        Returns:
            generated_texts (List[List[str]]): Texts generated by our models.
        """
        # vLLM configurations
        stop_tokens = ["<|endoftext|>", "Question"]
        sampling_params = SamplingParams(temperature=0., max_tokens=512, n=1, stop=stop_tokens)
        # vLLM inference
        outputs = self._generator.generate(data, sampling_params)
        # Output texts
        output_texts = [output.outputs[0].text for output in outputs]
        grouped_outputs = [
            output_texts[i : i + num_samples_per_prompt]
            for i in range(0, len(outputs), num_samples_per_prompt)
        ]
        return grouped_outputs

# True Evaluator
class PromptedJailbreakingEvaluator:
    """Evaluator for prompted jailbreaking attacks (Inference-Only)."""
    def __init__(
        self,
        task_lm: str,
        dataset: str,
        prompt: Optional[str],
        num_devices: int,
        placement_group: PlacementGroup,
        guard_model_name: str = "meta-llama/Llama-Guard-3-8B",
        priming: bool = False,
        pruning_metric: str = "ASR-EM"
    ):
        super().__init__()
        assert pruning_metric in ["ASR-EM", "ASR-LLM", "ASR-SV"], \
            "pruning_metric must be one of ['ASR-EM', 'ASR-LLM', 'ASR-SV']"
        self.dataset = dataset
        self.priming = priming
        self.task_lm = task_lm
        print("Task LM:", self.task_lm)
        self.pruning_metric = pruning_metric
        print("Fitness Metric:", self.pruning_metric)
        self.template = prompt or "{sentence_1} {prompt}"
        # Inference Model
        self._generator = JailbreakingPromptedModel.options(
            scheduling_strategy = PlacementGroupSchedulingStrategy(
                placement_group = placement_group,
                placement_group_bundle_index=0
            )
        ).remote(task_lm, dataset, num_devices)
        # Evaluation Metric Model
        self._evaluator = jailbreakingMetricEvaluator.options(
            scheduling_strategy = PlacementGroupSchedulingStrategy(
                placement_group = placement_group,
                placement_group_bundle_index=1
            )
        ).remote(guard_model_name)
        # Steering Vector (If specified)
        self._steering_evaluator = None
        if pruning_metric == "ASR-SV":
            self._steering_evaluator = SteeringVectorModelLayerWrapper.options(
            scheduling_strategy = PlacementGroupSchedulingStrategy(
                placement_group = placement_group,
                placement_group_bundle_index=2
            )
        ).remote(task_lm)

    @torch.no_grad()
    def forward(
        self,
        input_queries: List[str], # test_loader
        prompt: str,
        **kwargs
    ) -> Dict:
        """Only support vLLM here (Single Prompt Inference).
        
        Args:
            input_queries (List[str]): Original dataset's input queries.
            prompt (str): Input prompt (in-context attack or just priming).

        Returns:
            results: Dict, ASR-EM score (`EM_score`) & ASR-LLM score (`Guard_score`), both normalized.
        """ 
        # Data formatting
        data = [prompt.format(sentence_1=input_query) for input_query in input_queries]
        # Inference
        inference_futures = self._generator.forward.remote(data, **kwargs)
        inference_outputs = ray.get(inference_futures)
        # Evaluation
        evaluation_futures = self._evaluator.evaluate.remote(input_queries, inference_outputs)
        evaluation_results = ray.get(evaluation_futures)
        # Steering vector (In-case specified)
        if self.pruning_metric == "ASR-SV":
            sv_evaluation_futures = self._steering_evaluator.compute_steering_vector_fitness.remote(
                input_queries, prompt, 
            )
            sv_evaluation_result = ray.get(sv_evaluation_futures)
            evaluation_results["SV_score"] = sv_evaluation_result
            evaluation_results["reward"] = sv_evaluation_result
        else:
            evaluation_results["SV_score"] =  -1
            if self.pruning_metric == "ASR-EM":
                evaluation_results["reward"] = evaluation_results["EM_score"]
            elif self.pruning_metric == "ASR-LLM":
                evaluation_results["reward"] = evaluation_results["Guard_score"]
        
        if "Eval" in kwargs and kwargs["Eval"]:
            return evaluation_results, inference_outputs
        return evaluation_results

    @torch.no_grad()
    def forward_batch(
        self,
        input_queries: List[str],
        prompts: List[str],
        **kwargs
    ) -> Dict:
        """
        Batch evaluation using vLLM for multiple prompts.

        Args:
            input_queries (List[str]): Original dataset's input queries.
            prompts (List[str]): Input prompt list (in-context attack or just priming).

        Returns:
            results_list: List[Dict], ASR-EM score （`EM_score`） & ASR-LLM score (`Guard_score`), both normalized.
        """
        num_samples_per_prompt = len(input_queries)
        data = [
            p_template.replace("{sentence_1}", s_text)
            for p_template in prompts
            for s_text in input_queries
        ]
        # Inference
        inference_futures = self._generator.forward_batch.remote(data, num_samples_per_prompt, **kwargs)
        inference_outputs_list = ray.get(inference_futures)
        # Evaluation
        evaluation_futures = []
        for inference_outputs in inference_outputs_list:
            evaluation_future = self._evaluator.evaluate.remote(input_queries, inference_outputs)
            evaluation_futures.append(evaluation_future)
        evaluation_results_list = ray.get(evaluation_futures)
        # Steering vector (In-case specified)
        if self.pruning_metric == "ASR-SV":
            sv_evaluation_futures = []
            print(len(prompts))
            for prompt in prompts:
                sv_evaluation_future = self._steering_evaluator.compute_steering_vector_fitness.remote(
                    input_queries, prompt, 
                )
                sv_evaluation_futures.append(sv_evaluation_future)
            sv_evaluation_results = ray.get(sv_evaluation_futures)
            for i in range(len(evaluation_results_list)):
                evaluation_results_list[i]["SV_score"] = sv_evaluation_results[i]
                evaluation_results_list[i]["reward"] = sv_evaluation_results[i]
        else:
            for i in range(len(evaluation_results_list)):
                evaluation_results_list[i]["SV_score"] =  -1
                if self.pruning_metric == "ASR-EM":
                    evaluation_results_list[i]["reward"] = evaluation_results_list[i]["EM_score"]
                elif self.pruning_metric == "ASR-LLM":
                    evaluation_results_list[i]["reward"] = evaluation_results_list[i]["Guard_score"]
        if "Eval" in kwargs and kwargs["Eval"]:
            return evaluation_results, inference_outputs
        return evaluation_results_list


